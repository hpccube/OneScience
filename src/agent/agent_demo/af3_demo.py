import json
import os
import uuid

import json5
import requests
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print

the_user_id = {}


# Step 1 (Optional): Add some custom tools
@register_tool("af3_infer")
class af3_infer(BaseTool):
    description = "This function calculates the protein prediction results of alphafold3, inputs the json path, and returns the calculation results"
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [
        {
            "name": "input_json_file",
            "type": "str",
            "description": "the json file path of the alphafold3 input, string",
            "required": True,
        }
    ]

    def call(self, params: str, **kwargs) -> float:
        # `params` are the arguments generated by the LLM agent.
        server_http = "http://a02r2n13:8002/af3"
        input_json_file = json5.loads(params)["input_json_file"]
        user_id = the_user_id["first_user"]
        if os.path.exists(input_json_file) == False:
            return json5.dumps(
                {"state": "error", "result": "输入文件错误"}, ensure_ascii=False
            )
        with open(input_json_file, "r") as f:
            json_str = f.read()
        pass_json = json.loads(json_str)
        data = {"json_dict": pass_json, "user_id": user_id}
        resp = requests.post(server_http, json=data)
        return json5.dumps(resp.json(), ensure_ascii=False)


@register_tool("af3_show")
class af3_show(BaseTool):
    description = "This function visualizes protein prediction, inputs the file location to be returned, and returns an HTML file"
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [
        {
            "name": "file_location",
            "type": "str",
            "description": "the file location to be returned",
            "required": True,
        }
    ]

    def call(self, params: str, **kwargs) -> float:
        # `params` are the arguments generated by the LLM agent.
        server_http = "http://a02r2n13:8002/af3_save_result"
        file_location = json5.loads(params)["file_location"]
        user_id = the_user_id["first_user"]
        output_file = os.path.join(file_location, user_id + ".html")
        data = {"output_file": output_file, "user_id": user_id}
        resp = requests.post(server_http, json=data)
        return json5.dumps(resp.json(), ensure_ascii=False)


@register_tool("print_wait")
class af3_infer(BaseTool):
    description = "This function is always called before af3_infer"
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{}]

    def call(self, params: str, **kwargs) -> float:
        print("alphafold3正在推理，请等待")
        return json5.dumps({"output": "alphafold3正在推理，请等待"}, ensure_ascii=False)


# Step 2: Configure the LLM you are using.
llm_cfg = {
    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
    "model": "qwen7B",
    "model_server": "http://a02r2n06:8000/v1",  # base_url, also known as api_base
    "api_key": "EMPTY",
    # (Optional) LLM hyperparameters for generation:
    "generate_cfg": {"top_p": 0.8},
}

# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
system_instruction = """你是一个蛋白质预测的助手，你可以帮助用户使用alphafold3给用户预测蛋白质结构，你应当采取如下操作：
- 如果用户输入了一个json文件路径，请你提取这个本地文件路径作为参数传入af3_infer中，
- 如果需要进行af3_infer函数调用，请先调用print_wait工具，给用户返回打印结果，然后调用af3_infer并传入参数，
- 如果推理状态显示done，那么给用户返回蛋白质预测结果的路径，如果发生错误，那么给用户提示出错的信息
- 推理正常结束返回后，提示用户要不要返回可视化结果
- 如果用户要求返回可视化结果或者给出具体的位置路径，那么把可视化的路径本身作为参数调用可视化工具，注意不要使用最开始的蛋白质预测路径。
- 最终返回可视化保存的结果文件位置，即可视化工具返回的output_file字段。
"""
tools = [
    "af3_infer",
    "af3_show",
    "print_wait",
]  # `code_interpreter` is a built-in tool for executing code.
# files = ['./doc.pdf']  # Give the bot a PDF file to read.
bot = Assistant(
    llm=llm_cfg,
    system_message=system_instruction,
    function_list=tools,
)

# Step 4: Run the agent as a chatbot.
the_user_id["first_user"] = uuid.uuid4().hex[:10]
messages = []  # This stores the chat history.
while True:
    query = input("\nuser query: ")
    # Append the user query to the chat history.
    messages.append({"role": "user", "content": query})
    response = []
    response_plain_text = ""
    print("bot response:")
    for response in bot.run(messages=messages):
        # Streaming output.
        response_plain_text = typewriter_print(response, response_plain_text)
    # Append the bot responses to the chat history.
    messages.extend(response)
