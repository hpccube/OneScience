# U-Net

## Config file

The `config` directory contains many `yaml` config files with naming format `config_{1/2/3}D_{PDE name}.yaml` where all args for training and testing are saved. The explanations of some U-Net specific args are as follows:

* training args:
    * `training_type`: string, set 'autoregressive' for autoregressive trainging using autoregressive loss or 'single' for single step training using single step loss.
    * `pushforward`: bool, set 'True' for pushforward training. And `training_type` also must be set to true at the same time.
    * `initial_step`: int, the number of input time steps. (default: 10)
    * `unroll_step`: int, the number of time steps to backpropagate in the pushforward training. (default: 20)

* model args:
    * `in_channels`: int, the number of input channels that equals to the number of variables to be solved. For example, there are 3 variables to be solved for 1D Compressible NS equation: density, pressure and velocity.
    * `out_channels`: int, the number of output channels that equals to the `in_channels`.
    * `init_features`: int, the number of channels in the 1st upsample block of U-Net.

We believe that the meaning of other args is explicit. For reproducibility, we give our training hyperparameters for solving different PDEs in the table below.

| PDE Name                    | spatial resolution / downsample rate | temporal resolution / downsample rate | lr    | epochs | batch size | weight decay | initial step | unroll step |
| :-------------------------- | :---------------------- | :----------------------- | :---- | :----- | :--------- | :----------- | :----------- | :---------- |
| 1D Advection                | 1024/4                  | 201/5                    | 1.e-3 | 500    | 64         | 1.e-4        | 10           | 20          |
| 1D Diffusion-Reaction       | 1024/4                  | 101/1                    | 1.e-3 | 500    | 64         | 1.e-4        | 10           | 20          |
| 1D Burgers                  | 1024/4                  | 201/5                    | 1.e-3 | 500    | 64         | 1.e-4        | 10           | 20          |
| 1D Diffusion-Sorption       | 1024/4                  | 101/1                    | 1.e-3 | 500    | 64         | 1.e-4        | 10           | 20          |
| 1D Allen Cahn               | 1024/4                  | 101/1                    | 1.e-3 | 500    | 64         | 1.e-4        | 10           | 20          |
| 1D Cahn Hilliard            | 1024/4                  | 101/1                    | 1.e-3 | 500    | 64         | 1.e-4        | 10           | 20          |
| 1D Compressible NS          | 1024/4                  | 101/1                    | 1.e-3 | 500    | 64         | 1.e-4        | 10           | 20          |
| 2D Burgers                  | 128/1                   | 101/1                    | 1.e-3 | 500    | 8          | 1.e-4        | 10           | 20          |
| 2D Compressible NS          | 128/2                   | 21/1                     | 1.e-3 | 500    | 32         | 1.e-4        | 10           | 20          |
| 2D DarcyFlow                | 128/1                   | -                        | 1.e-3 | 500    | 64         | 1.e-4        | 1            | 1           |
| 2D Shallow Water            | 128/1                   | 101/1                    | 1.e-3 | 500    | 8          | 1.e-4        | 10           | 20          |
| 2D Allen Cahn               | 128/1                   | 101/1                    | 1.e-3 | 500    | 8          | 1.e-4        | 10           | 20          |
| 2D Black-Scholes-Barenblatt | 128/1                   | 101/1                    | 1.e-3 | 500    | 8          | 1.e-4        | 10           | 20          |
| 3D Compressible NS          | 128/2                   | 21/1                     | 1.e-3 | 500    | 2          | 1.e-4        | 10           | 20          |
| 3D Eular                    | 128/2                   | 21/1                     | 1.e-3 | 500    | 2          | 1.e-4        | 10           | 20          |
| 3D Maxwell                  | 32                      | 8                        | 1.e-3 | 500    | 2          | 1.e-4        | 2            | -           |

## Loss function

U-Net solves PDEs in autoregressive manner where model $f_{\theta}$ predicts the solution of next time steps $\hat{u}^{k+1}$ based on the solution of previous $l$ (`initial_step`) time steps $\{\hat{u}^{k-l+
1},...,\hat{u}^k\}$. The process can be formalized as

$$
\hat{u}^{k+1} = f_{\theta}(\hat{u}^{k-l+1},...,\hat{u}^k).
$$

The loss function has the form:

$$
\mathcal{L}=\frac{1}{N}\sum_{k=0}^{N-1}l(f_{\theta}(\hat{u}^{k-l+1},...,\hat{u}^k), u^{k+1})
$$ 

where $u^{k+1}$ is ground truth data generated by high-order numerical methods and $l$ is MSE in our implementation.

In practice, the soltuions of first $l$ time steps denoted as $\{u^0,...,u^{l-1}\}$ may be generated by other high-order methods, which are the initial model input. The solutions of remaining time steps are generated by trained model autoregressivly. And the input of remaining time steps will consists of model prediction.

## Training strategy

**Single step training**: The input to model always comes from ground truth data. So that the loss function has the form:

$$
\mathcal{L}=\frac{1}{N}\sum_{k=0}^{N-1}l(f_{\theta}(u^{k-l+1},...,u^k), u^{k+1}).
$$

**Autoregressive training**: Standard training strategy. In addition to the initial input comes from ground truth data, the intermediate inputs comes from model prediction. The loss function has the same form as above.

**Pushforward trick**: This trick is proposed in [MPNN](https://arxiv.org/abs/2202.03376) to encourage more stable and faster autoregressive training. With pushforward trick, we only backpropagate gradient on the last $M$ (`unroll_step`) time steps . We take the implementation from [PDEBench](https://arxiv.org/abs/2210.07182). The loss function has the form:

$$
\mathcal{L}=\frac{1}{M}\sum_{k=N-M}^{N-1}l(f_{\theta}(u^{k-l+1},...,u^k), u^{k+1}).
$$

## Train

1. Check the following args in the config file:
    1. The path of `file_name` and `saved_folder` are correct;
    2. `if_training` is `True`;
2. Set hyperparameters for training, such as `lr`, `batch size`, etc. You can use the default values we provide;
3. Run command:
```bash
CUDA_VISIBLE_DEVICES=0 python train.py ./config/${config file name}
# Example: CUDA_VISIBLE_DEVICES=0 python train.py ./configs/config_1D_Advection.yaml
```

## Resume training

1. Modify config file:
    1. Make sure `if_training` is `True`;
    2. Set `continue_training` to `True`;
    3. Set `model_path` to the checkpoint path where traing restart;
2. Run command:
```bash
CUDA_VISIBLE_DEVICES=0 python train.py ./config/${config file name}
```

## Test

1. Modify config file:
    1. Set `if_training` to `False`;
    2. Set `model_path` to the checkpoint path where the model to be evaluated is saved.
2. Run command:
```bash
CUDA_VISIBLE_DEVICES=0 python train.py ./config/${config file name}
```

## Estimate lipschitz upper bound

1. Set `model_path` to the checkpoint path where the model to be evaluated is saved.
2. Compute Singular value decomposition (SVD) for model. Run command:
    ```bash
    python get_model_sv.py ./configs/${config file name} ${directory path to save results}
    ```
3. Estimate lipschitz upper bound using SeqLip algorithm. Run command:
    ```bash
    python get_model_sv.py ./configs/${config file name} ${directory path to save results} --n_sv 1
    ```