graphcast:
  # ┌───────────────────────────────────────────┐
  # │            Model Configuration            │
  # └───────────────────────────────────────────┘

  processor_layers: 16              # Number of processor layers.
  hidden_dim: 512                   # Size of each layer.
  mesh_level: 6                     # Max icosphere level in the multimesh.
  multimesh: true                   # If true, uses multimesh for the processor.
  img_size: [721, 1440]     # Resolution of the latitude-longitude grid. If smaller than the native resolution, bilinear interpolation is applied.
  num_channels_climate: 69    # Number of climate channels.
  use_cos_zenith: true        # If true, uses cosine zenith angle as additional channel(s). It can replace the total incident solar radiation.
  use_time_of_year_index: true  # If true, the dataloader also gives the index of the sample for calculating the time of day and year progress.
  num_history: 0              # Number of historical (previous time steps) to use. With history=1, the model uses t-1 and t to predict t+1.
  num_channels_static: 5      # Number of static channels (e.g., land-sea mask, geopotential, cosine of latitudes, sine and cosine of longitudes).
  num_steps: 1

  checkpoint_encoder: true         # If true, applies single-segment gradient checkpointing for the embedder, encoder, and the first layer of the processor combined.
  checkpoint_decoder: false        # If true, applies single-segment gradient checkpointing for the last layer of the processor, decoder, and the final layer combined.

  full_bf16: true             # If true, uses bfloat16 for the entire training.

  processor_type: MessagePassing    # "GraphTransformer" as in GenCast, or "MessagePassing" as in GraphCast.
  khop_neighbors: 32                # Number of neighbors for each node used in the GraphTransformer. Only used if the processor type is "GraphTransformer".
  num_attention_heads: 4            # Number of attention heads. Only used if the processor type is "GraphTransformer".
  norm_type: LayerNorm            # "TELayerNorm" or "LayerNorm". Use "TELayerNorm" for improved performance.

  segments: 1                      # Number of segments in gradient checkpointing for the processor. Only used if "checkpoint_processor" is true.
  force_single_checkpoint: false   # If true, applies single-segment end-to-end gradient checkpointing.

  checkpoint_processor: false      # If true, applies gradient checkpointing for the processor, excluding first and last layers. "segments" controls the number of segments for gradient checkpointing.

  force_single_checkpoint_finetune: false   # If true, applies single-segment end-to-end gradient checkpointing for fine-tuning (multi-step rollout).
  checkpoint_encoder_finetune: true         # If true, applies single-segment gradient checkpointing for fine-tuning for the embedder, encoder, and the first layer of the processor combined.
  checkpoint_processor_finetune: true       # If true, applies gradient checkpointing for fine-tuning for the processor, excluding first and last layers. "segments" controls the number of segments for gradient checkpointing.
  checkpoint_decoder_finetune: true         # If true, applies single-segment gradient checkpointing for fine-tuning for the last layer of the processor, decoder, and the final layer combined.
  concat_trick: true          # If true, uses a concatenation trick to reduce memory overhead and improve MLP layer performance on the source, destination node features, and edge features.
  # See https://docs.dgl.ai/guide/message-efficient.html for more info.
  cugraphops_encoder: false   # If true, uses cugraphops backend for the encoder.
  cugraphops_processor: false # If true, uses cugraphops backend for the processor.
  cugraphops_decoder: false   # If true, uses cugraphops backend for the decoder.
  recompute_activation: false  # If true, recomputes activation in backward to save memory. Currently, only SiLU is supported.
  use_apex: true              # If true, uses Apex for fused Adam optimizer, typically resulting in 10-20% faster training iterations.

  # ┌───────────────────────────────────────────┐
  # │           Dataset Configuration           │
  # └───────────────────────────────────────────┘
  start_epoch: 0
  max_epoch: 1000
  train_data_dir: '/work/share/ac8hkycjba/osdatasets/graphcast/train/'            # Path to the dataset.
  val_data_dir: '/work/share/ac8hkycjba/osdatasets/graphcast/val/'          # Path to the dataset.
  test_data_dir: '/work/share/ac8hkycjba/osdatasets/graphcast/test/'          # Path to the dataset.
  stats_dir: "/work/share/ac8hkycjba/osdatasets/graphcast/stats/"                # Path to the static datasets. Includes .nc files for land-sea mask and geopotential.
  static_dataset_path: "/work/share/ac8hkycjba/osdatasets/graphcast/static/"
  checkpoint_dir : "./checkpoints"
  dataset_metadata_path: './data.json'  # Path to the dataset metadata, containing channel names.
  time_diff_std_path: './time_diff_std.npy'  # Path to the .npy file with standard deviation of normalized per-variable per-pressure level time differences.
  channels:  ['10m_u_component_of_wind', '10m_v_component_of_wind', '2m_temperature', 'mean_sea_level_pressure',

              'geopotential_1000', 'geopotential_925', 'geopotential_850', 'geopotential_700', 'geopotential_600', 'geopotential_500',
              'geopotential_400', 'geopotential_300', 'geopotential_250', 'geopotential_200', 'geopotential_150', 'geopotential_100', 'geopotential_50',

              'specific_humidity_1000', 'specific_humidity_925', 'specific_humidity_850',
              'specific_humidity_700', 'specific_humidity_600',
              'specific_humidity_500', 'specific_humidity_400', 'specific_humidity_300', 'specific_humidity_250', 'specific_humidity_200',
              'specific_humidity_150', 'specific_humidity_100', 'specific_humidity_50',

              'temperature_1000', 'temperature_925', 'temperature_850', 'temperature_700', 'temperature_600', 'temperature_500', 'temperature_400',
              'temperature_300', 'temperature_250', 'temperature_200','temperature_150', 'temperature_100', 'temperature_50',

              'u_component_of_wind_1000', 'u_component_of_wind_925', 'u_component_of_wind_850', 'u_component_of_wind_700', 'u_component_of_wind_600',
              'u_component_of_wind_500', 'u_component_of_wind_400', 'u_component_of_wind_300', 'u_component_of_wind_250', 'u_component_of_wind_200',
              'u_component_of_wind_150', 'u_component_of_wind_100', 'u_component_of_wind_50',

              'v_component_of_wind_1000', 'v_component_of_wind_925', 'v_component_of_wind_850', 'v_component_of_wind_700', 'v_component_of_wind_600',
              'v_component_of_wind_500', 'v_component_of_wind_400', 'v_component_of_wind_300', 'v_component_of_wind_250', 'v_component_of_wind_200',
              'v_component_of_wind_150', 'v_component_of_wind_100', 'v_component_of_wind_50'
  ]
  time_res: 6
  batch_size: 1
  patience: 30
  num_workers: 1              # Number of subprocesses to use for data loading. 0 means data is loaded in the main process.
  num_val_steps: 8            # Number of rollouts used in light-weight validation during training.
  dt: 6.0                     # Time in hours between each timestep in the dataset. A dt of 6.0 means four timesteps per day.
  grad_clip_norm: 32.0        # Threshold for gradient clipping.
  lr: 1e-3                    # Max learning rate in the learning rate schedule. Starts from zero to "lr" within "num_iters_step1" steps, then decays with a cosine schedule in "num_iters_step2" steps, reaching "lr_step3".
  lr_step3: 3e-7              # Min learning rate in the learning rate schedule.
  num_iters_step1: 1000       # Number of iterations (backward passes) in the first phase of the learning rate schedule.
  num_iters_step2: 299000     # Number of iterations (backward passes) in the second phase of the learning rate schedule.
  num_iters_step3: 11000      # Number of iterations (backward passes) for incremental fine-tuning, with increments of "step_change_freq".
  step_change_freq: 1000      # Frequency of increments for multi-step fine-tuning.
  val_freq: 5                 # Frequency (iterations) for performing light-weight validation during training.
